{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176dde8-bf27-4941-82d6-f27357e15098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap environment: clone AGILE repo, create conda env, install dependencies\n",
    "import os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "lnpdb_base = Path(\"../LNPDB/data/LNPDB_For_AGILE\")\n",
    "agile_dir = lnpdb_base / \"AGILE\"\n",
    "\n",
    "# Clone AGILE if not already cloned\n",
    "if not agile_dir.exists():\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/bowang-lab/AGILE\", str(agile_dir)])\n",
    "\n",
    "# Create conda env 'agile' (run in terminal once, not inside notebook)\n",
    "!conda create -n agile python=3.9 -y\n",
    "!conda activate agile\n",
    "\n",
    "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install torch-geometric==2.2.0 torch-sparse==0.6.16 torch-scatter==2.1.0 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install -r requirements.txt\n",
    "!conda install -c conda-forge rdkit pandas numpy tqdm -y\n",
    "!pip install mordred\n",
    "!git clone https://github.com/NVIDIA/apex.git && cd apex && pip install -v --no-build-isolation --disable-pip-version-check ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d12f28-0a1b-4a4b-a1a4-a4dae12b06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages and basic function(s)\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import logging\n",
    "from multiprocessing import freeze_support\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "freeze_support()\n",
    "\n",
    "def safe_div(value, factor, feature_name, printed_flag):\n",
    "    try:\n",
    "        return value / np.float64(factor)\n",
    "    except Exception:\n",
    "        # only print the first time this feature fails\n",
    "        if not printed_flag[0]:\n",
    "            print(f\"{feature_name} unable to be calculated, setting invalid values to 0\")\n",
    "            printed_flag[0] = True\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438e9bb-3153-4680-80bd-c8a19f50f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy finetune script + yaml into AGILE, and patch YAML per split\n",
    "scripts_dir = lnpdb_base / \"scripts\"\n",
    "finetune_py = scripts_dir / \"finetune_LNPDB.py\"\n",
    "finetune_yaml_template = scripts_dir / \"config_finetune.yaml\"\n",
    "\n",
    "# Ensure files exist\n",
    "assert finetune_py.exists(), \"finetune_LNPDB.py not found in scripts/\"\n",
    "assert finetune_yaml_template.exists(), \"config_finetune.yaml not found in scripts/\"\n",
    "\n",
    "# Copy finetune_LNPDB.py into AGILE\n",
    "shutil.copy(finetune_py, agile_dir / \"finetune_LNPDB.py\")\n",
    "\n",
    "# Generate one YAML per split\n",
    "finetune_config_dir = agile_dir / \"finetune\"\n",
    "finetune_config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i in range(5):\n",
    "    yaml_target = finetune_config_dir / f\"agile_lnp_hela_cv_{i}.yaml\"\n",
    "    with open(finetune_yaml_template, \"r\") as f:\n",
    "        yaml_text = f.read()\n",
    "    yaml_text = yaml_text.replace(\"task_name: lnp_hela_with_feat\", f\"task_name: LNPDB_split_{i}\")\n",
    "    with open(yaml_target, \"w\") as f:\n",
    "        f.write(yaml_text)\n",
    "\n",
    "logging.info(\"finetune_LNPDB.py and split YAMLs prepared in AGILE/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9985e14-19db-45b4-8bd9-d51f5a476c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split AGILE data (Morgan fingerprints included)\n",
    "random.seed(0)\n",
    "no_splits = 5\n",
    "\n",
    "# read from ../LNPDB/data/LNPDB_for_AGILE/data\n",
    "df = pd.read_csv(\"../LNPDB/data/LNPDB_for_AGILE/AGILE/data/finetuning_set_smiles_plus_features.csv\")\n",
    "\n",
    "def create_df(name):\n",
    "    name = pd.DataFrame()\n",
    "    return name\n",
    "\n",
    "output_dataframes=[]\n",
    "for i in range(no_splits):\n",
    "    output_dataframes.append(create_df(f\"df{i}\"))\n",
    "\n",
    "complement_dataframes=[]\n",
    "for k in range(no_splits):\n",
    "    complement_dataframes.append(create_df(f\"df{k}c\"))\n",
    "\n",
    "for index in range(len(df)):\n",
    "    row = df.iloc[[index]]\n",
    "    assignment = random.randint(0,no_splits-1)\n",
    "    while len(output_dataframes[assignment])>=len(df)/no_splits:\n",
    "        assignment = random.randint(0,no_splits-1)\n",
    "    output_dataframes[assignment] = pd.concat([output_dataframes[assignment], row], ignore_index=True)\n",
    "    for complement_assignment in range(no_splits):\n",
    "        if complement_assignment != assignment:\n",
    "            complement_dataframes[complement_assignment] = pd.concat([complement_dataframes[complement_assignment], row], ignore_index=True)\n",
    "\n",
    "# save to ../LNPDB/data/LNPDB_for_AGILE/cv_splits\n",
    "cv_dir = Path(\"../LNPDB/data/LNPDB_for_AGILE/cv_splits\")\n",
    "cv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for j, dataframe in enumerate(output_dataframes):\n",
    "    df_split.to_csv(cv_dir / f\"df{j}_test.csv\", index=False)\n",
    "\n",
    "for k, dataframe in enumerate(complement_dataframes):\n",
    "    df_comp.to_csv(cv_dir / f\"df{k}_train.csv\", index=False)\n",
    "\n",
    "logging.info(\"Cross-validation splits saved to cv_splits/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ff506-202c-4b1b-8e83-b6b31f38a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy finetune_LNPDB.py from ../LNPDB/data/LNPDB_for_AGILE/scripts into ../LNPDB/data/LNPDB_for_AGILE/AGILE\n",
    "# also copy yamls in similar manner\n",
    "\n",
    "# finetune pre-trained AGILE model on 5 cross-validation splits\n",
    "cv_number_list = [0,1,2,3,4]\n",
    "\n",
    "for i in cv_number_list:\n",
    "    yaml_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{i}.yaml\"\n",
    "    command = [\"python\", str(agile_dir / \"finetune_LNPDB.py\"), str(yaml_path)]\n",
    "    result = subprocess.run(command, cwd=agile_dir, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42da57a-10af-4094-b889-ed7ce4ac16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: hardcode which csvs into infer_vis\n",
    "\n",
    "# Copy infer_vis_LNPDB.py and infer yaml into AGILE model folders\n",
    "infer_py = scripts_dir / \"infer_vis_LNPDB.py\"\n",
    "infer_yaml_template = scripts_dir / \"infer_vis/config_finetune.yaml\"\n",
    "\n",
    "# Ensure files exist\n",
    "assert infer_py.exists(), \"infer_vis_LNPDB.py not found in scripts/\"\n",
    "assert infer_yaml_template.exists(), \"infer_vis/config_finetune.yaml not found in scripts/\"\n",
    "\n",
    "# Copy infer_vis_LNPDB.py into AGILE\n",
    "shutil.copy(infer_py, agile_dir / \"infer_vis_LNPDB.py\")\n",
    "\n",
    "# # Copy yaml into each split's checkpoints folder\n",
    "# for i in range(5):\n",
    "#     ckpt_dir = agile_dir / f\"finetune/agile_lnp_hela_cv_{i}/checkpoints\"\n",
    "#     ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     shutil.copy(infer_yaml_template, ckpt_dir / \"config_finetune.yaml\")\n",
    "\n",
    "logging.info(\"infer_vis_LNPDB.py copied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d0088-3f07-4964-88cd-c065b9b6e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Mordred descriptors for heldout datasets and align with known features\n",
    "def heldout_data(heldout_data_path: str,\n",
    "                 heldout_data_output_folder: str,\n",
    "                 known_data_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate Mordred descriptors for molecules in heldout_data, then select only\n",
    "    those features that appear in known_data and append them to the heldout_data.\n",
    "    Saves both the full descriptor table and the filtered heldout_data to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - heldout_data_path: path to CSV with columns ['smiles', ...]\n",
    "    - heldout_data_output_folder: folder to create for output files\n",
    "    - known_data_path: path to CSV whose columns define the desired features\n",
    "    \"\"\"\n",
    "\n",
    "    df_heldout = pd.read_csv(heldout_data_path)\n",
    "    logging.info(f\"Read heldout data with {len(df_heldout)} molecules.\")\n",
    "\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in df_heldout['IL_SMILES']]\n",
    "    calc = Calculator(descriptors, ignore_3D=True)\n",
    "    df_desc = calc.pandas(mols, nproc=1)\n",
    "\n",
    "    # clean columns\n",
    "    if isinstance(df_desc.columns, pd.MultiIndex):\n",
    "        df_desc.columns = [str(name) for name, _ in df_desc.columns]\n",
    "    else:\n",
    "        df_desc.columns = [str(c) for c in df_desc.columns]\n",
    "\n",
    "    df_desc.replace([np.inf, -np.inf], pd.NA, inplace=True)\n",
    "\n",
    "    if df_desc.isna().sum().sum() > 0:\n",
    "        logging.warning(f\"Detected {df_desc.isna().sum().sum()} NaN or overflow values after descriptor calculation.\")\n",
    "\n",
    "    os.makedirs(heldout_data_output_folder, exist_ok=True)\n",
    "    dataset_name = Path(heldout_data_path).stem\n",
    "\n",
    "    desc_full_path = os.path.join(heldout_data_output_folder,\n",
    "                                  f\"{dataset_name}_descriptors_full.csv\")\n",
    "    df_desc.to_csv(desc_full_path, index=False)\n",
    "    logging.info(f\"Full descriptor table saved to {desc_full_path}.\")\n",
    "\n",
    "    df_known = pd.read_csv(known_data_path)\n",
    "    desired_cols = [col for col in df_known.columns if col.startswith('desc_')]\n",
    "    logging.info(f\"Found {len(desired_cols)} desired desc_* columns in known data.\")\n",
    "\n",
    "    # collect matching columns\n",
    "    new_features = {}\n",
    "    missing_features = []\n",
    "    overflow_features = []\n",
    "    # Extract feature names from desired columns\n",
    "    scaling_factor = None\n",
    "    for col in desired_cols:\n",
    "        if '/' in col:\n",
    "            feature_name, scaling_factor = col[len('desc_'):].split('/')\n",
    "        else:\n",
    "            feature_name = col[len('desc_'):] \n",
    "\n",
    "        if feature_name in df_desc.columns:\n",
    "            feature_values = df_desc[feature_name]\n",
    "            printed_flag = [False]\n",
    "            if feature_values.isna().any():\n",
    "                logging.warning(f\"Feature '{feature_name}' contains NaNs due to overflow during calculation.\")\n",
    "                overflow_features.append(feature_name)\n",
    "            # else if feature_name == 'MAXssNH' or feature_name == 'MINssNH':\n",
    "            elif scaling_factor:\n",
    "                new_features[col] = [\n",
    "                    safe_div(value, scaling_factor, feature_name, printed_flag)\n",
    "                    for value in feature_values.values\n",
    "                ]\n",
    "            else:\n",
    "                new_features[col] = [\n",
    "                    safe_div(value, 1, feature_name, printed_flag)\n",
    "                    for value in feature_values.values\n",
    "                ]\n",
    "            logging.info(f\"Added feature column '{col}' from descriptor '{feature_name}'.\")\n",
    "        else:\n",
    "            if feature_name == 'log_VR1_A' or feature_name == 'log_VR2_A' or feature_name == 'log_SdssC':\n",
    "                feature_values = df_desc[f'{feature_name[4:]}']\n",
    "                if feature_values.isna().any():\n",
    "                    logging.warning(f\"Feature '{feature_name}' contains NaNs due to overflow during calculation.\")\n",
    "                    overflow_features.append(f'log_{feature_name}')\n",
    "                new_features[col] = [math.log10(value) if value>0 else 0 for value in feature_values.values]\n",
    "                logging.info(f\"Added feature column '{col}' from descriptor '{feature_name}'.\")\n",
    "            else:\n",
    "                logging.warning(f\"Descriptor '{feature_name}' not found in calculated descriptors. Filling NaNs.\")\n",
    "                new_features[col] = [pd.NA] * len(df_heldout)\n",
    "                missing_features.append(feature_name)\n",
    "\n",
    "        scaling_factor = None\n",
    "\n",
    "    df_new_features = pd.DataFrame(new_features)\n",
    "    df_new_features = df_new_features.reindex(columns=desired_cols)\n",
    "    df_heldout = pd.concat([df_heldout.reset_index(drop=True),\n",
    "                            df_new_features.reset_index(drop=True)], axis=1)\n",
    "    df_heldout.rename(columns={'IL_SMILES': 'smiles'}, inplace=True)\n",
    "\n",
    "    heldout_out_path = os.path.join(heldout_data_output_folder, f\"{dataset_name}_plus_features.csv\")\n",
    "    df_heldout.to_csv(heldout_out_path, index=False)\n",
    "\n",
    "    logging.info(f\"Augmented heldout data saved to {heldout_out_path}\")\n",
    "\n",
    "    # Warn about missing features if any\n",
    "    if missing_features:\n",
    "        logging.warning(f\"{len(missing_features)} features were missing: {missing_features}\")\n",
    "\n",
    "    if overflow_features:\n",
    "        logging.warning(f\"{len(overflow_features)} features contain NaN values due to overflow: {overflow_features}\")\n",
    "\n",
    "\n",
    "# Compare generated Mordred descriptors vs. known features, with summary stats\n",
    "def compare_descriptors(generated_path: str, known_path: str, n_check: int = 5,\n",
    "                        atol: float = 1e-6, rtol: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Compare a generated Mordred descriptor CSV to a known training feature CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_path: path to heldout_data_output.csv (with descriptors)\n",
    "    - known_path: path to finetuning_set_smiles_plus_features.csv\n",
    "    - n_check: number of molecules to sample for detailed row comparison\n",
    "    - atol, rtol: tolerances for numerical equality (absolute, relative)\n",
    "    \"\"\"\n",
    "    df_gen = pd.read_csv(generated_path)\n",
    "    df_known = pd.read_csv(known_path)\n",
    "\n",
    "    # Select only descriptor columns (start with desc_)\n",
    "    gen_cols = [c for c in df_gen.columns if c.startswith(\"desc_\")]\n",
    "    known_cols = [c for c in df_known.columns if c.startswith(\"desc_\")]\n",
    "\n",
    "    # Check overlap\n",
    "    overlap = sorted(set(gen_cols) & set(known_cols))\n",
    "    missing_in_gen = set(known_cols) - set(gen_cols)\n",
    "    extra_in_gen = set(gen_cols) - set(known_cols)\n",
    "\n",
    "    print(f\"Total known features: {len(known_cols)}\")\n",
    "    print(f\"Total generated features: {len(gen_cols)}\")\n",
    "    print(f\"Overlap: {len(overlap)}\")\n",
    "    if missing_in_gen:\n",
    "        print(f\"❌ Missing in generated: {sorted(list(missing_in_gen))[:10]} ...\")\n",
    "    if extra_in_gen:\n",
    "        print(f\"⚠️ Extra in generated: {sorted(list(extra_in_gen))[:10]} ...\")\n",
    "\n",
    "    # Early exit if no overlap\n",
    "    if not overlap:\n",
    "        print(\"No overlapping descriptors to compare.\")\n",
    "        return\n",
    "\n",
    "    # Align dataframes to overlap\n",
    "    df_gen_overlap = df_gen[overlap].reset_index(drop=True)\n",
    "    df_known_overlap = df_known[overlap].reset_index(drop=True)\n",
    "\n",
    "    # Check shape match\n",
    "    n_rows = min(len(df_gen_overlap), len(df_known_overlap))\n",
    "    df_gen_overlap = df_gen_overlap.iloc[:n_rows]\n",
    "    df_known_overlap = df_known_overlap.iloc[:n_rows]\n",
    "\n",
    "    # Compare all values\n",
    "    diffs_mask = ~np.isclose(df_gen_overlap.values,\n",
    "                             df_known_overlap.values,\n",
    "                             atol=atol, rtol=rtol, equal_nan=True)\n",
    "\n",
    "    n_total = diffs_mask.size\n",
    "    n_diffs = np.count_nonzero(diffs_mask)\n",
    "    pct_match = 100 * (1 - n_diffs / n_total)\n",
    "\n",
    "    # Molecule-level agreement\n",
    "    row_match = np.all(~diffs_mask, axis=1)\n",
    "    pct_rows_match = 100 * np.mean(row_match)\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Total values compared: {n_total}\")\n",
    "    print(f\"- Mismatched values: {n_diffs}\")\n",
    "    print(f\"- % values matching: {pct_match:.2f}%\")\n",
    "    print(f\"- % molecules with all descriptors matching: {pct_rows_match:.2f}%\")\n",
    "\n",
    "    # Detailed check on sample rows\n",
    "    import random\n",
    "    sample_idx = random.sample(range(n_rows), min(n_check, n_rows))\n",
    "    print(f\"\\nChecking {len(sample_idx)} random molecules for descriptor agreement:\")\n",
    "    for idx in sample_idx:\n",
    "        diffs = []\n",
    "        for j, col in enumerate(overlap):\n",
    "            val_gen = df_gen_overlap.iloc[idx, j]\n",
    "            val_known = df_known_overlap.iloc[idx, j]\n",
    "            if not np.isclose(val_gen, val_known, atol=atol, rtol=rtol, equal_nan=True):\n",
    "                diffs.append((col, val_gen, val_known))\n",
    "        if diffs:\n",
    "            print(f\"Row {idx}: {len(diffs)} differences, e.g. {diffs[:5]}\")\n",
    "        else:\n",
    "            print(f\"Row {idx}: ✅ all overlapping descriptors match\")\n",
    "\n",
    "# Example call (edit datasets as needed)\n",
    "heldout_data(\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/AGILE_data/finetuning_set_smiles_plus_features.csv\", # only change this argument for each dataset\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/outputs\",\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/AGILE_data/finetuning_set_smiles_plus_features.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# verify mordred generation algorithm on original AGILE data\n",
    "compare_descriptors(\n",
    "    \"../LNPDB/data/LNPDB_For_AGILE/outputs/finetuning_set_smiles_plus_features_plus_features.csv\",\n",
    "    \"../LNPDB/data/LNPDB_For_AGILE/AGILE/data/finetuning_set_smiles_plus_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b926e6b-c185-4ab4-bb92-2e4753d6c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do cross-validation on the test splits\n",
    "def test_split_validation() -> None:\n",
    "    \"\"\"\n",
    "    Run the AGILE cross-evaluation splits on respective test splits\n",
    "    \"\"\"\n",
    "    cv_number_list = [0, 1, 2, 3, 4]\n",
    "    # run infer_vis\n",
    "    for cv_number in cv_number_list:\n",
    "        model_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}\"\n",
    "        yaml_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/checkpoints/config_finetune.yaml\"\n",
    "        # replace yaml value based on dataset_name\n",
    "        with open(yaml_path, \"r\") as f:\n",
    "            yaml_text = f.read()\n",
    "        yaml_text = yaml_text.replace(\"task_name: lnp_hela_with_feat\", f\"task_name: df{cv_number}_test\")\n",
    "        with open(yaml_path, \"w\") as f:\n",
    "            f.write(yaml_text)\n",
    "        command = [\"python\", str(agile_dir / \"infer_vis_LNPDB.py\"), str(model_path)]\n",
    "        result = subprocess.run(command, cwd=agile_dir, capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "\n",
    "        # TODO: update directory??\n",
    "        # preds_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/LNPDB/preds_on_LNPDB.csv\"\n",
    "        # if preds_path.exists():\n",
    "        #     df_preds = pd.read_csv(preds_path)\n",
    "        #     save_dir = Path(dataset_name)\n",
    "        #     save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        #     save_preds_path = save_dir / f\"{dataset_name}_preds_on_LNPDB_cv_{cv_number}.csv\"\n",
    "        #     df_preds.to_csv(save_preds_path, index=False)\n",
    "        #     logging.info(f\"Predictions for split {cv_number} saved to {save_preds_path}\")\n",
    "\n",
    "test_split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42ce43-1f57-4719-9de6-954c92fc6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate cross-validation splits on dataset\n",
    "def cross_evaluate() -> None:\n",
    "    \"\"\"\n",
    "    Run the AGILE cross-evaluation splits on LNPDB data\n",
    "    \"\"\"\n",
    "    cv_number_list = [0, 1, 2, 3, 4]\n",
    "    # datasets to evaluate\n",
    "    datasets = [\"SL_2020_heldout_data\"]\n",
    "    for dataset_name of datasets:\n",
    "        # run infer_vis\n",
    "        for cv_number in cv_number_list:\n",
    "            model_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}\"\n",
    "            yaml_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/checkpoints/config_finetune.yaml\"\n",
    "            # replace yaml value based on dataset_name\n",
    "            with open(yaml_path, \"r\") as f:\n",
    "                yaml_text = f.read()\n",
    "            yaml_text = yaml_text.replace(\"task_name: lnp_hela_with_feat\", f\"task_name: {dataset_name}\")\n",
    "            with open(yaml_path, \"w\") as f:\n",
    "                f.write(yaml_text)\n",
    "            command = [\"python\", str(agile_dir / \"infer_vis_LNPDB.py\"), str(model_path)]\n",
    "            result = subprocess.run(command, cwd=agile_dir, capture_output=True, text=True)\n",
    "            print(result.stdout)\n",
    "            print(result.stderr)\n",
    "\n",
    "            # TODO: update directory??\n",
    "            # preds_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/LNPDB/preds_on_LNPDB.csv\"\n",
    "            # if preds_path.exists():\n",
    "            #     df_preds = pd.read_csv(preds_path)\n",
    "            #     save_dir = Path(dataset_name)\n",
    "            #     save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            #     save_preds_path = save_dir / f\"{dataset_name}_preds_on_LNPDB_cv_{cv_number}.csv\"\n",
    "            #     df_preds.to_csv(save_preds_path, index=False)\n",
    "            #     logging.info(f\"Predictions for split {cv_number} saved to {save_preds_path}\")\n",
    "\n",
    "cross_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
