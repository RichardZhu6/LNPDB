{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de80bb9-077f-43a3-9f7b-ed41042f8583",
   "metadata": {},
   "source": [
    "# 1. Installation of requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d12f28-0a1b-4a4b-a1a4-a4dae12b06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages and basic function(s)\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import logging\n",
    "from multiprocessing import freeze_support\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "freeze_support()\n",
    "\n",
    "def safe_div(value, factor, feature_name, printed_flag):\n",
    "    try:\n",
    "        return value / np.float64(factor)\n",
    "    except Exception:\n",
    "        # only print the first time this feature fails\n",
    "        if not printed_flag[0]:\n",
    "            print(f\"{feature_name} unable to be calculated, setting invalid values to 0\")\n",
    "            printed_flag[0] = True\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980fbfb-8c3a-45ce-b9b6-4706b806c3f5",
   "metadata": {},
   "source": [
    "# 2. Splitting data into test/train splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d3bbf-7e20-4a8f-8e88-39d4fcceac5e",
   "metadata": {},
   "source": [
    "\n",
    "The pre-trained AGILE deep learning model is provided in `AGILE/ckpt/pretrained_agile_60k` and will be fine-tuned on five cross-validation splits.\n",
    "\n",
    "The data provided in [repo](https://github.com/bowang-lab/AGILE) was split (80% train/20% validation) randomly to create these splits.\n",
    "\n",
    "In this section we will split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9985e14-19db-45b4-8bd9-d51f5a476c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split AGILE data (Morgan fingerprints included)\n",
    "random.seed(0)\n",
    "no_splits = 5\n",
    "\n",
    "# read from ../LNPDB/data/LNPDB_for_AGILE/data\n",
    "df = pd.read_csv(\"../LNPDB/data/LNPDB_for_AGILE/AGILE/data/finetuning_set_smiles_plus_features.csv\")\n",
    "\n",
    "def create_df(name):\n",
    "    name = pd.DataFrame()\n",
    "    return name\n",
    "\n",
    "output_dataframes=[]\n",
    "for i in range(no_splits):\n",
    "    output_dataframes.append(create_df(f\"df{i}\"))\n",
    "\n",
    "complement_dataframes=[]\n",
    "for k in range(no_splits):\n",
    "    complement_dataframes.append(create_df(f\"df{k}c\"))\n",
    "\n",
    "for index in range(len(df)):\n",
    "    row = df.iloc[[index]]\n",
    "    assignment = random.randint(0,no_splits-1)\n",
    "    while len(output_dataframes[assignment])>=len(df)/no_splits:\n",
    "        assignment = random.randint(0,no_splits-1)\n",
    "    output_dataframes[assignment] = pd.concat([output_dataframes[assignment], row], ignore_index=True)\n",
    "    for complement_assignment in range(no_splits):\n",
    "        if complement_assignment != assignment:\n",
    "            complement_dataframes[complement_assignment] = pd.concat([complement_dataframes[complement_assignment], row], ignore_index=True)\n",
    "\n",
    "# save to ../LNPDB/data/LNPDB_for_AGILE/cv_splits\n",
    "cv_dir = Path(\"../LNPDB/data/LNPDB_for_AGILE/cv_splits\")\n",
    "cv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for j, dataframe in enumerate(output_dataframes):\n",
    "    df_split.to_csv(cv_dir / f\"df{j}_test.csv\", index=False)\n",
    "\n",
    "for k, dataframe in enumerate(complement_dataframes):\n",
    "    df_comp.to_csv(cv_dir / f\"df{k}_train.csv\", index=False)\n",
    "\n",
    "logging.info(\"Cross-validation splits saved to cv_splits/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18502eb8-024e-4c66-96cc-72583efc024d",
   "metadata": {},
   "source": [
    "# 3. Finetuning models on cross-validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19d62f-2e87-4914-b946-4855b13f3621",
   "metadata": {},
   "source": [
    "The following section involves preparing files according to [AGILE](https://github.com/bowang-lab/AGILE).\n",
    "    \n",
    "Note that the trained model checkpoints are already provided at `LNPDB/data/LNPDB_for_AGILE/cv_splits`, so it is not necessary to run the commands.\n",
    "\n",
    "The models and their results are now placed in `LNPDB/data/LNPDB_for_AGILE/cv_splits`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b85df9-8360-4d7c-874d-12e80ebdc8f9",
   "metadata": {},
   "source": [
    "## 3.1 Preparing finetune_LNPDB.py and YAML/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438e9bb-3153-4680-80bd-c8a19f50f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy finetune script + yaml into AGILE, and patch YAML per split\n",
    "scripts_dir = lnpdb_base / \"scripts\"\n",
    "finetune_py = scripts_dir / \"finetune_LNPDB.py\"\n",
    "finetune_yaml_template = scripts_dir / \"config_finetune.yaml\"\n",
    "\n",
    "# Ensure files exist\n",
    "assert finetune_py.exists(), \"finetune_LNPDB.py not found in scripts/\"\n",
    "assert finetune_yaml_template.exists(), \"config_finetune.yaml not found in scripts/\"\n",
    "\n",
    "# Copy finetune_LNPDB.py into AGILE\n",
    "shutil.copy(finetune_py, agile_dir / \"finetune_LNPDB.py\")\n",
    "\n",
    "# Generate one YAML per split\n",
    "finetune_config_dir = agile_dir / \"finetune\"\n",
    "finetune_config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i in range(5):\n",
    "    yaml_target = finetune_config_dir / f\"agile_lnp_hela_cv_{i}.yaml\"\n",
    "    with open(finetune_yaml_template, \"r\") as f:\n",
    "        yaml_text = f.read()\n",
    "    yaml_text = yaml_text.replace(\"task_name: lnp_hela_with_feat\", f\"task_name: LNPDB_split_{i}\")\n",
    "    with open(yaml_target, \"w\") as f:\n",
    "        f.write(yaml_text)\n",
    "\n",
    "logging.info(\"finetune_LNPDB.py and split YAMLs prepared in AGILE/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61fbbd5-9053-4eaa-b0ab-c898a6504772",
   "metadata": {},
   "source": [
    "## 3.2 Finetuning models on respective split train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ff506-202c-4b1b-8e83-b6b31f38a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy finetune_LNPDB.py from ../LNPDB/data/LNPDB_for_AGILE/scripts into ../LNPDB/data/LNPDB_for_AGILE/AGILE\n",
    "# also copy yamls in similar manner\n",
    "\n",
    "# finetune pre-trained AGILE model on 5 cross-validation splits\n",
    "cv_number_list = [0,1,2,3,4]\n",
    "\n",
    "for i in cv_number_list:\n",
    "    yaml_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{i}.yaml\"\n",
    "    command = [\"python\", str(agile_dir / \"finetune_LNPDB.py\"), str(yaml_path)]\n",
    "    result = subprocess.run(command, cwd=agile_dir, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba06bb0-c3b2-4622-8931-7ad445647ec1",
   "metadata": {},
   "source": [
    "# 4. Generating LNPDB data feature descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2173d8fc-fa2a-4936-aad0-676c2b4f2755",
   "metadata": {},
   "source": [
    "To use the trained models to predict delivery efficacy for new LNP data, LNPDB data has been placed into the folder `LNPDB/data/LNPDB_for_AGILE/LNPDB_data`.\n",
    "\n",
    "AGILE requires data to be processed into Mordred molecular feature descriptors, which are generated using [repo](https://github.com/mordred-descriptor/mordred) as described in the sixth cell of the notebook. For all LNPDB data included in the paper, this repository already contains the corresponding Mordred descriptors in `LNPDB/data/LNPDB_for_AGILE/LNPDB_data`.\n",
    "\n",
    "Our Mordred descriptors were validated by comparing generation from AGILE original SMILES and provided fingerprints. The procedure is to generate Mordred descriptors using solely the smiles and Experiment_value columns, and then compare the resulting descriptors with those provided by the AGILE [repo](https://github.com/bowang-lab/AGILE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585d586-48ef-4e89-a1fa-d3e94cd21bdd",
   "metadata": {},
   "source": [
    "## 4.1 Generate descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d0088-3f07-4964-88cd-c065b9b6e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Mordred descriptors for heldout datasets and align with known features\n",
    "def heldout_data(heldout_data_path: str,\n",
    "                 heldout_data_output_folder: str,\n",
    "                 known_data_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate Mordred descriptors for molecules in heldout_data, then select only\n",
    "    those features that appear in known_data and append them to the heldout_data.\n",
    "    Saves both the full descriptor table and the filtered heldout_data to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - heldout_data_path: path to CSV with columns ['smiles', ...]\n",
    "    - heldout_data_output_folder: folder to create for output files\n",
    "    - known_data_path: path to CSV whose columns define the desired features\n",
    "    \"\"\"\n",
    "\n",
    "    df_heldout = pd.read_csv(heldout_data_path)\n",
    "    logging.info(f\"Read heldout data with {len(df_heldout)} molecules.\")\n",
    "\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in df_heldout['IL_SMILES']]\n",
    "    calc = Calculator(descriptors, ignore_3D=True)\n",
    "    df_desc = calc.pandas(mols, nproc=1)\n",
    "\n",
    "    # clean columns\n",
    "    if isinstance(df_desc.columns, pd.MultiIndex):\n",
    "        df_desc.columns = [str(name) for name, _ in df_desc.columns]\n",
    "    else:\n",
    "        df_desc.columns = [str(c) for c in df_desc.columns]\n",
    "\n",
    "    df_desc.replace([np.inf, -np.inf], pd.NA, inplace=True)\n",
    "\n",
    "    if df_desc.isna().sum().sum() > 0:\n",
    "        logging.warning(f\"Detected {df_desc.isna().sum().sum()} NaN or overflow values after descriptor calculation.\")\n",
    "\n",
    "    os.makedirs(heldout_data_output_folder, exist_ok=True)\n",
    "    dataset_name = Path(heldout_data_path).stem\n",
    "\n",
    "    desc_full_path = os.path.join(heldout_data_output_folder,\n",
    "                                  f\"{dataset_name}_descriptors_full.csv\")\n",
    "    df_desc.to_csv(desc_full_path, index=False)\n",
    "    logging.info(f\"Full descriptor table saved to {desc_full_path}.\")\n",
    "\n",
    "    df_known = pd.read_csv(known_data_path)\n",
    "    desired_cols = [col for col in df_known.columns if col.startswith('desc_')]\n",
    "    logging.info(f\"Found {len(desired_cols)} desired desc_* columns in known data.\")\n",
    "\n",
    "    # collect matching columns\n",
    "    new_features = {}\n",
    "    missing_features = []\n",
    "    overflow_features = []\n",
    "    # Extract feature names from desired columns\n",
    "    scaling_factor = None\n",
    "    for col in desired_cols:\n",
    "        # some features used by AGILE are scaled\n",
    "        if '/' in col:\n",
    "            feature_name, scaling_factor = col[len('desc_'):].split('/')\n",
    "        else:\n",
    "            feature_name = col[len('desc_'):] \n",
    "\n",
    "        if feature_name in df_desc.columns:\n",
    "            feature_values = df_desc[feature_name]\n",
    "            printed_flag = [False]\n",
    "            if feature_values.isna().any():\n",
    "                logging.warning(f\"Feature '{feature_name}' contains NaNs due to overflow during calculation.\")\n",
    "                overflow_features.append(feature_name)\n",
    "            # else if feature_name == 'MAXssNH' or feature_name == 'MINssNH':\n",
    "            elif scaling_factor:\n",
    "                new_features[col] = [\n",
    "                    safe_div(value, scaling_factor, feature_name, printed_flag)\n",
    "                    for value in feature_values.values\n",
    "                ]\n",
    "            else:\n",
    "                new_features[col] = [\n",
    "                    safe_div(value, 1, feature_name, printed_flag)\n",
    "                    for value in feature_values.values\n",
    "                ]\n",
    "            logging.info(f\"Added feature column '{col}' from descriptor '{feature_name}'.\")\n",
    "        else:\n",
    "            # some features used by AGILE are log-Mordred features\n",
    "            if feature_name == 'log_VR1_A' or feature_name == 'log_VR2_A' or feature_name == 'log_SdssC':\n",
    "                feature_values = df_desc[f'{feature_name[4:]}']\n",
    "                if feature_values.isna().any():\n",
    "                    logging.warning(f\"Feature '{feature_name}' contains NaNs due to overflow during calculation.\")\n",
    "                    overflow_features.append(f'log_{feature_name}')\n",
    "                new_features[col] = [math.log10(value) if value>0 else 0 for value in feature_values.values]\n",
    "                logging.info(f\"Added feature column '{col}' from descriptor '{feature_name}'.\")\n",
    "            else:\n",
    "                logging.warning(f\"Descriptor '{feature_name}' not found in calculated descriptors. Filling NaNs.\")\n",
    "                new_features[col] = [pd.NA] * len(df_heldout)\n",
    "                missing_features.append(feature_name)\n",
    "\n",
    "        scaling_factor = None\n",
    "\n",
    "    df_new_features = pd.DataFrame(new_features)\n",
    "    df_new_features = df_new_features.reindex(columns=desired_cols)\n",
    "    df_heldout = pd.concat([df_heldout.reset_index(drop=True),\n",
    "                            df_new_features.reset_index(drop=True)], axis=1)\n",
    "    df_heldout.rename(columns={'IL_SMILES': 'smiles'}, inplace=True)\n",
    "\n",
    "    heldout_out_path = os.path.join(heldout_data_output_folder, f\"{dataset_name}_plus_features.csv\")\n",
    "    df_heldout.to_csv(heldout_out_path, index=False)\n",
    "\n",
    "    logging.info(f\"Augmented heldout data saved to {heldout_out_path}\")\n",
    "\n",
    "    # Warn about missing features if any\n",
    "    if missing_features:\n",
    "        logging.warning(f\"{len(missing_features)} features were missing: {missing_features}\")\n",
    "\n",
    "    if overflow_features:\n",
    "        logging.warning(f\"{len(overflow_features)} features contain NaN values due to overflow: {overflow_features}\")\n",
    "\n",
    "# Example call (edit datasets as needed)\n",
    "heldout_data(\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/AGILE_data/finetuning_set_smiles_plus_features.csv\", # only change this argument for each dataset\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/outputs\",\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/AGILE_data/finetuning_set_smiles_plus_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac3450-5bff-42e7-b3dc-e3ce7401f48a",
   "metadata": {},
   "source": [
    "## 4.2 Validate descriptor generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266646aa-83d9-4381-9907-a8ebffeb5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generated Mordred descriptors vs. known features, with summary stats\n",
    "def compare_descriptors(generated_path: str, known_path: str, n_check: int = 5,\n",
    "                        atol: float = 1e-6, rtol: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Compare a generated Mordred descriptor CSV to a known training feature CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_path: path to heldout_data_output.csv (with descriptors)\n",
    "    - known_path: path to finetuning_set_smiles_plus_features.csv\n",
    "    - n_check: number of molecules to sample for detailed row comparison\n",
    "    - atol, rtol: tolerances for numerical equality (absolute, relative)\n",
    "    \"\"\"\n",
    "    df_gen = pd.read_csv(generated_path)\n",
    "    df_known = pd.read_csv(known_path)\n",
    "\n",
    "    # Select only descriptor columns (start with desc_)\n",
    "    gen_cols = [c for c in df_gen.columns if c.startswith(\"desc_\")]\n",
    "    known_cols = [c for c in df_known.columns if c.startswith(\"desc_\")]\n",
    "\n",
    "    # Check overlap\n",
    "    overlap = sorted(set(gen_cols) & set(known_cols))\n",
    "    missing_in_gen = set(known_cols) - set(gen_cols)\n",
    "    extra_in_gen = set(gen_cols) - set(known_cols)\n",
    "\n",
    "    print(f\"Total known features: {len(known_cols)}\")\n",
    "    print(f\"Total generated features: {len(gen_cols)}\")\n",
    "    print(f\"Overlap: {len(overlap)}\")\n",
    "    if missing_in_gen:\n",
    "        print(f\"❌ Missing in generated: {sorted(list(missing_in_gen))[:10]} ...\")\n",
    "    if extra_in_gen:\n",
    "        print(f\"⚠️ Extra in generated: {sorted(list(extra_in_gen))[:10]} ...\")\n",
    "\n",
    "    # Early exit if no overlap\n",
    "    if not overlap:\n",
    "        print(\"No overlapping descriptors to compare.\")\n",
    "        return\n",
    "\n",
    "    # Align dataframes to overlap\n",
    "    df_gen_overlap = df_gen[overlap].reset_index(drop=True)\n",
    "    df_known_overlap = df_known[overlap].reset_index(drop=True)\n",
    "\n",
    "    # Check shape match\n",
    "    n_rows = min(len(df_gen_overlap), len(df_known_overlap))\n",
    "    df_gen_overlap = df_gen_overlap.iloc[:n_rows]\n",
    "    df_known_overlap = df_known_overlap.iloc[:n_rows]\n",
    "\n",
    "    # Compare all values\n",
    "    diffs_mask = ~np.isclose(df_gen_overlap.values,\n",
    "                             df_known_overlap.values,\n",
    "                             atol=atol, rtol=rtol, equal_nan=True)\n",
    "\n",
    "    n_total = diffs_mask.size\n",
    "    n_diffs = np.count_nonzero(diffs_mask)\n",
    "    pct_match = 100 * (1 - n_diffs / n_total)\n",
    "\n",
    "    # Molecule-level agreement\n",
    "    row_match = np.all(~diffs_mask, axis=1)\n",
    "    pct_rows_match = 100 * np.mean(row_match)\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Total values compared: {n_total}\")\n",
    "    print(f\"- Mismatched values: {n_diffs}\")\n",
    "    print(f\"- % values matching: {pct_match:.2f}%\")\n",
    "    print(f\"- % molecules with all descriptors matching: {pct_rows_match:.2f}%\")\n",
    "\n",
    "    # Detailed check on sample rows\n",
    "    import random\n",
    "    sample_idx = random.sample(range(n_rows), min(n_check, n_rows))\n",
    "    print(f\"\\nChecking {len(sample_idx)} random molecules for descriptor agreement:\")\n",
    "    for idx in sample_idx:\n",
    "        diffs = []\n",
    "        for j, col in enumerate(overlap):\n",
    "            val_gen = df_gen_overlap.iloc[idx, j]\n",
    "            val_known = df_known_overlap.iloc[idx, j]\n",
    "            if not np.isclose(val_gen, val_known, atol=atol, rtol=rtol, equal_nan=True):\n",
    "                diffs.append((col, val_gen, val_known))\n",
    "        if diffs:\n",
    "            print(f\"Row {idx}: {len(diffs)} differences, e.g. {diffs[:5]}\")\n",
    "        else:\n",
    "            print(f\"Row {idx}: ✅ all overlapping descriptors match\")\n",
    "\n",
    "\n",
    "# generate features using original AGILE data\n",
    "heldout_data(\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/AGILE_data/finetuning_set_smiles_plus_features.csv\", # only change this argument for each dataset\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/outputs\",\n",
    "    \"../LNPDB/data/LNPDB_for_AGILE/AGILE_data/finetuning_set_smiles_plus_features.csv\"\n",
    ")\n",
    "            \n",
    "# verify mordred generation algorithm on original AGILE data\n",
    "compare_descriptors(\n",
    "    \"../LNPDB/data/LNPDB_For_AGILE/outputs/finetuning_set_smiles_plus_features_plus_features.csv\",\n",
    "    \"../LNPDB/data/LNPDB_For_AGILE/AGILE/data/finetuning_set_smiles_plus_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20034ca3-5f32-4ae4-9c79-056317fd8036",
   "metadata": {},
   "source": [
    "# 5. Evaluating models on AGILE and LNPDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665811e7-95ef-4cc5-93db-097bece74fc6",
   "metadata": {},
   "source": [
    "Once the molecular feature descriptors are generated, AGILE splits can make predictions on delivery efficacy for LNPDB data.\n",
    "\n",
    "The following blocks move`infer_vis_LNPDB.py` into AGILE from `LNPDB/data/LNPDB_for_AGILE/scripts`, evaluate the fine-tuned AGILE models on the test cross-evaluation splits and make predictions on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be968627-2764-4a2d-8788-dddb7139dd1b",
   "metadata": {},
   "source": [
    "## 5.1 Preparing infer_vis_LNPDB.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42da57a-10af-4094-b889-ed7ce4ac16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: hardcode which csvs into infer_vis\n",
    "\n",
    "# Copy infer_vis_LNPDB.py and infer yaml into AGILE model folders\n",
    "infer_py = scripts_dir / \"infer_vis_LNPDB.py\"\n",
    "infer_yaml_template = scripts_dir / \"infer_vis/config_finetune.yaml\"\n",
    "\n",
    "# Ensure files exist\n",
    "assert infer_py.exists(), \"infer_vis_LNPDB.py not found in scripts/\"\n",
    "assert infer_yaml_template.exists(), \"infer_vis/config_finetune.yaml not found in scripts/\"\n",
    "\n",
    "# Copy infer_vis_LNPDB.py into AGILE\n",
    "shutil.copy(infer_py, agile_dir / \"infer_vis_LNPDB.py\")\n",
    "\n",
    "# # Copy yaml into each split's checkpoints folder\n",
    "# for i in range(5):\n",
    "#     ckpt_dir = agile_dir / f\"finetune/agile_lnp_hela_cv_{i}/checkpoints\"\n",
    "#     ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     shutil.copy(infer_yaml_template, ckpt_dir / \"config_finetune.yaml\")\n",
    "\n",
    "logging.info(\"infer_vis_LNPDB.py copied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2666e82-b121-4794-9ad0-bc88a5647c58",
   "metadata": {},
   "source": [
    "## 5.2 Evaluate fine-tuned models on test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b926e6b-c185-4ab4-bb92-2e4753d6c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do cross-validation on the test splits\n",
    "def test_split_validation() -> None:\n",
    "    \"\"\"\n",
    "    Run the AGILE cross-evaluation splits on respective test splits\n",
    "    \"\"\"\n",
    "    cv_number_list = [0, 1, 2, 3, 4]\n",
    "    # run infer_vis\n",
    "    for cv_number in cv_number_list:\n",
    "        model_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}\"\n",
    "        yaml_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/checkpoints/config_finetune.yaml\"\n",
    "        # replace yaml value based on dataset_name\n",
    "        with open(yaml_path, \"r\") as f:\n",
    "            yaml_text = f.read()\n",
    "        yaml_text = yaml_text.replace(\"task_name: lnp_hela_with_feat\", f\"task_name: df{cv_number}_test\")\n",
    "        with open(yaml_path, \"w\") as f:\n",
    "            f.write(yaml_text)\n",
    "        command = [\"python\", str(agile_dir / \"infer_vis_LNPDB.py\"), str(model_path)]\n",
    "        result = subprocess.run(command, cwd=agile_dir, capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "\n",
    "        # TODO: update directory??\n",
    "        # preds_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/LNPDB/preds_on_LNPDB.csv\"\n",
    "        # if preds_path.exists():\n",
    "        #     df_preds = pd.read_csv(preds_path)\n",
    "        #     save_dir = Path(dataset_name)\n",
    "        #     save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        #     save_preds_path = save_dir / f\"{dataset_name}_preds_on_LNPDB_cv_{cv_number}.csv\"\n",
    "        #     df_preds.to_csv(save_preds_path, index=False)\n",
    "        #     logging.info(f\"Predictions for split {cv_number} saved to {save_preds_path}\")\n",
    "\n",
    "test_split_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3fe74-01c8-4453-b96f-cfc3282f8151",
   "metadata": {},
   "source": [
    "## 5.3 Make predictions on LNPDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42ce43-1f57-4719-9de6-954c92fc6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate cross-validation splits on dataset\n",
    "def cross_evaluate() -> None:\n",
    "    \"\"\"\n",
    "    Run the AGILE cross-evaluation splits on LNPDB data\n",
    "    \"\"\"\n",
    "    cv_number_list = [0, 1, 2, 3, 4]\n",
    "    # datasets to evaluate\n",
    "    datasets = [\"SL_2020_heldout_data\"]\n",
    "    for dataset_name of datasets:\n",
    "        # run infer_vis\n",
    "        for cv_number in cv_number_list:\n",
    "            model_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}\"\n",
    "            yaml_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/checkpoints/config_finetune.yaml\"\n",
    "            # replace yaml value based on dataset_name\n",
    "            with open(yaml_path, \"r\") as f:\n",
    "                yaml_text = f.read()\n",
    "            yaml_text = yaml_text.replace(\"task_name: lnp_hela_with_feat\", f\"task_name: {dataset_name}\")\n",
    "            with open(yaml_path, \"w\") as f:\n",
    "                f.write(yaml_text)\n",
    "            command = [\"python\", str(agile_dir / \"infer_vis_LNPDB.py\"), str(model_path)]\n",
    "            result = subprocess.run(command, cwd=agile_dir, capture_output=True, text=True)\n",
    "            print(result.stdout)\n",
    "            print(result.stderr)\n",
    "\n",
    "            # TODO: update directory??\n",
    "            # preds_path = agile_dir / f\"finetune/agile_lnp_hela_cv_{cv_number}/LNPDB/preds_on_LNPDB.csv\"\n",
    "            # if preds_path.exists():\n",
    "            #     df_preds = pd.read_csv(preds_path)\n",
    "            #     save_dir = Path(dataset_name)\n",
    "            #     save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            #     save_preds_path = save_dir / f\"{dataset_name}_preds_on_LNPDB_cv_{cv_number}.csv\"\n",
    "            #     df_preds.to_csv(save_preds_path, index=False)\n",
    "            #     logging.info(f\"Predictions for split {cv_number} saved to {save_preds_path}\")\n",
    "\n",
    "cross_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
